{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alixintong/btt-ai-know-your-customer/blob/main/pix2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmldhj5qiuIW",
        "outputId": "39fefcd1-76d0-4354-8072-ad5590058c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# this cell allows access to our folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "root_path = '/content/gdrive/MyDrive'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/BTTAI_AmericanExpress1/pix2seq\")"
      ],
      "metadata": {
        "id": "IUVEldkQoaMz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0s9QksAnkHg3",
        "outputId": "a6a2168a-e64f-45f8-c39a-43c2e969dfbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/.shortcut-targets-by-id/17C-FeV89lWfoxThc1mu8REz8adMDfNfP/BTTAI_AmericanExpress1/pix2seq'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2fD5EB4jNRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "630a597d-526a-4132-8c36-83a1e70099d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pix2seq'...\n",
            "remote: Enumerating objects: 137, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 137 (delta 23), reused 18 (delta 18), pack-reused 108\u001b[K\n",
            "Receiving objects: 100% (137/137), 14.29 MiB | 19.03 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n"
          ]
        }
      ],
      "source": [
        "#!git clone https://github.com/google-research/pix2seq.git\n",
        "#!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### training (fine-tuning) of object detection models"
      ],
      "metadata": {
        "id": "xe-7u1cqlcQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "#!unzip annotations_trainval2017.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwu57QVol2uU",
        "outputId": "e301a1df-a99f-4740-ea80-e100d191c5e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-29 16:45:32--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.6.124\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.6.124|:80... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "Archive:  annotations_trainval2017.zip\n",
            "replace annotations/instances_train2017.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --mode=train --model_dir=/tmp/model_dir --config=configs/config_det_finetune.py --config.dataset.coco_annotations_dir=annotations --config.train.batch_size=32 --config.train.epochs=20 --config.optimization.learning_rate=3e-5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByaMgcjTkLYi",
        "outputId": "04d3fa6c-c6ac-449d-bdd3-a8666f50c258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-10-29 16:46:06.506357: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I1029 16:46:06.512140 140424874329984 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I1029 16:46:06.512633 140424874329984 utils.py:241] Running using MirroredStrategy on 1 replicas\n",
            "I1029 16:46:06.512913 140424874329984 utils.py:301] Config: dataset:\n",
            "  batch_duplicates: 1\n",
            "  cache_dataset: false\n",
            "  coco_annotations_dir: annotations\n",
            "  eval_split: validation\n",
            "  label_shift: 0\n",
            "  name: object_detection\n",
            "  tfds_name: coco/2017\n",
            "  train_filename: instances_train2017.json\n",
            "  train_split: train\n",
            "  val_filename: instances_val2017.json\n",
            "datasets:\n",
            "- !!python/object:ml_collections.config_dict.config_dict.ConfigDict\n",
            "  _convert_dict: true\n",
            "  _fields:\n",
            "    batch_duplicates: 1\n",
            "    cache_dataset: false\n",
            "    coco_annotations_dir: annotations\n",
            "    eval_split: validation\n",
            "    label_shift: 0\n",
            "    name: object_detection\n",
            "    tfds_name: coco/2017\n",
            "    train_filename: instances_train2017.json\n",
            "    train_split: train\n",
            "    val_filename: instances_val2017.json\n",
            "  _locked: false\n",
            "  _type_safe: true\n",
            "eval:\n",
            "  batch_size: 8\n",
            "  checkpoint_dir: ''\n",
            "  steps: 0\n",
            "  tag: eval\n",
            "model:\n",
            "  coord_vocab_shift: 1000\n",
            "  dec_proj_mode: mlp\n",
            "  decoder_output_bias: true\n",
            "  dim_att: 768\n",
            "  dim_att_dec: 512\n",
            "  dim_mlp: 3072\n",
            "  dim_mlp_dec: 2048\n",
            "  drop_att: 0.0\n",
            "  drop_path: 0.1\n",
            "  drop_units: 0.1\n",
            "  image_size: 640\n",
            "  max_seq_len: 512\n",
            "  name: encoder_ar_decoder\n",
            "  num_decoder_layers: 6\n",
            "  num_encoder_layers: 12\n",
            "  num_heads: 12\n",
            "  num_heads_dec: 16\n",
            "  patch_size: 16\n",
            "  pos_encoding: sin_cos\n",
            "  pos_encoding_dec: learned\n",
            "  pretrained_ckpt: gs://pix2seq/obj365_pretrain/vit_b_640x640_b256_s400k\n",
            "  resnet_variant: c1\n",
            "  shared_decoder_embedding: true\n",
            "  text_vocab_shift: 3000\n",
            "  use_cls_token: false\n",
            "  vocab_size: 3000\n",
            "model_dir: /tmp/model_dir\n",
            "optimization:\n",
            "  beta1: 0.9\n",
            "  beta2: 0.95\n",
            "  end_lr_factor: 0.01\n",
            "  eps: 1.0e-08\n",
            "  global_clipnorm: -1\n",
            "  learning_rate: 3.0e-05\n",
            "  learning_rate_scaling: none\n",
            "  learning_rate_schedule: linear\n",
            "  optimizer: adamw\n",
            "  warmup_epochs: 2\n",
            "  warmup_steps: 0\n",
            "  weight_decay: 0.05\n",
            "task:\n",
            "  class_label_corruption: rand_n_fake_cls\n",
            "  color_jitter_strength: 0.0\n",
            "  eos_token_weight: 0.1\n",
            "  image_size: 640\n",
            "  jitter_scale_max: 2.0\n",
            "  jitter_scale_min: 0.3\n",
            "  max_instances_per_image: 100\n",
            "  max_instances_per_image_test: 100\n",
            "  name: object_detection\n",
            "  noise_bbox_weight: 1.0\n",
            "  object_order: random\n",
            "  quantization_bins: 1000\n",
            "  temperature: 1.0\n",
            "  top_k: 0\n",
            "  top_p: 0.4\n",
            "  vocab_id: 10\n",
            "  weight: 1.0\n",
            "tasks:\n",
            "- !!python/object:ml_collections.config_dict.config_dict.ConfigDict\n",
            "  _convert_dict: true\n",
            "  _fields:\n",
            "    class_label_corruption: rand_n_fake_cls\n",
            "    color_jitter_strength: 0.0\n",
            "    eos_token_weight: 0.1\n",
            "    image_size: 640\n",
            "    jitter_scale_max: 2.0\n",
            "    jitter_scale_min: 0.3\n",
            "    max_instances_per_image: 100\n",
            "    max_instances_per_image_test: 100\n",
            "    name: object_detection\n",
            "    noise_bbox_weight: 1.0\n",
            "    object_order: random\n",
            "    quantization_bins: 1000\n",
            "    temperature: 1.0\n",
            "    top_k: 0\n",
            "    top_p: 0.4\n",
            "    vocab_id: 10\n",
            "    weight: 1.0\n",
            "  _locked: false\n",
            "  _type_safe: true\n",
            "train:\n",
            "  batch_size: 32\n",
            "  checkpoint_epochs: 1\n",
            "  checkpoint_steps: 0\n",
            "  epochs: 20\n",
            "  keep_checkpoint_max: 5\n",
            "  loss_type: xent\n",
            "  steps: 0\n",
            "\n",
            "loading annotations into memory...\n",
            "Done (t=14.91s)\n",
            "creating index...\n",
            "index created!\n",
            "2022-10-29 16:46:39.178058: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"NOT_FOUND: Error executing an HTTP request: HTTP response code 404\".\n",
            "I1029 16:46:39.293284 140424874329984 dataset_info.py:582] Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: coco/2017/1.1.0\n",
            "I1029 16:46:39.512963 140424874329984 dataset_info.py:491] Load dataset info from /tmp/tmp3awpd2o4tfds\n",
            "I1029 16:46:39.518295 140424874329984 dataset_info.py:552] Field info.description from disk and from code do not match. Keeping the one from code.\n",
            "I1029 16:46:39.518435 140424874329984 dataset_info.py:552] Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
            "I1029 16:46:39.518502 140424874329984 dataset_info.py:552] Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
            "I1029 16:46:39.518568 140424874329984 dataset_info.py:552] Field info.citation from disk and from code do not match. Keeping the one from code.\n",
            "I1029 16:46:39.518691 140424874329984 dataset_info.py:552] Field info.splits from disk and from code do not match. Keeping the one from code.\n",
            "I1029 16:46:39.518763 140424874329984 dataset_info.py:552] Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
            "I1029 16:46:39.519309 140424874329984 dataset_builder.py:437] Generating dataset coco (~/tensorflow_datasets/coco/2017/1.1.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"run.py\", line 252, in <module>\n",
            "    app.run(main)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 308, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 254, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"run.py\", line 215, in main\n",
            "    task, dataset = get_task_and_dataset(task_config)\n",
            "  File \"run.py\", line 66, in get_task_and_dataset\n",
            "    dataset = dataset_lib.DatasetRegistry.lookup(config.dataset.name)(config)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/17C-FeV89lWfoxThc1mu8REz8adMDfNfP/BTTAI_AmericanExpress1/pix2seq/data/coco.py\", line 42, in __init__\n",
            "    super().__init__(config)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/17C-FeV89lWfoxThc1mu8REz8adMDfNfP/BTTAI_AmericanExpress1/pix2seq/data/dataset.py\", line 48, in __init__\n",
            "    self.builder.download_and_prepare()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\", line 446, in download_and_prepare\n",
            "    self.info.dataset_size,\n",
            "OSError: Not enough disk space. Needed: 25.20 GiB (download: 25.20 GiB, generated: Unknown size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluation of object detection models"
      ],
      "metadata": {
        "id": "NFPx5FvWmEJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --mode=eval --model_dir=/tmp/model_dir --config=configs/config_det_finetune.py --config.dataset.coco_annotations_dir=annotations --config.eval.batch_size=40"
      ],
      "metadata": {
        "id": "h3Y0xp1jlnIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H2AZ5YgsmguI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Another implementation of Pix2Seq\n",
        "https://github.com/moein-shariatnia/Pix2Seq"
      ],
      "metadata": {
        "id": "glxM1xqUrEAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "5ctQ94Jx2WzO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#os.chdir(\"/content/drive/MyDrive/BTTAI_AmericanExpress1/\")\n",
        "#!git clone https://github.com/moein-shariatnia/Pix2Seq.git"
      ],
      "metadata": {
        "id": "jfhzDz99rLAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/BTTAI_AmericanExpress1/Pix2Seq\")"
      ],
      "metadata": {
        "id": "ixcGKHGstwaS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the model locally"
      ],
      "metadata": {
        "id": "A1onKxyRuCQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !chmod 755 download_data.sh\n",
        "# ! ./download_data.sh"
      ],
      "metadata": {
        "id": "jT9jJVrLvEnQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall scikit-learn -y\n",
        "# !sudo apt-get install python3.8\n",
        "# !sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\n",
        "# !sudo update-alternatives --config python3\n",
        "# !python3 --version\n",
        "# !sudo apt install python3-pip\n",
        "# !python -m pip install --upgrade pip\n",
        "# !pip install -U scikit-learn==1.1.2"
      ],
      "metadata": {
        "id": "a9h-Pmm31C8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xcVIC9dcuz9I",
        "outputId": "76a68682-3a4d-45e6-aa25-3880454200ce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting albumentations==1.2.1\n",
            "  Downloading albumentations-1.2.1-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.7/116.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting map_boxes==1.0.5\n",
            "  Using cached map_boxes-1.0.5-py3-none-any.whl (5.1 kB)\n",
            "Collecting matplotlib==3.5.1\n",
            "  Downloading matplotlib-3.5.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.21.4\n",
            "  Downloading numpy-1.21.4-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.3.5\n",
            "  Downloading pandas-1.3.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit_learn==1.1.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (1.1.2)\n",
            "Collecting timm==0.6.7\n",
            "  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==1.10.0\n",
            "  Downloading torch-1.10.0-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m875.9/881.9 MB\u001b[0m \u001b[31m153.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1102381056 bytes == 0x371c2000 @  0x7f5f073db615 0x5d631c 0x51e4f1 0x51e67b 0x4f7585 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x5d7c18 0x49ca7c 0x55e571 0x5d7cf1 0x49ced5 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.62.3\n",
            "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.21.1\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from albumentations==1.2.1->-r requirements.txt (line 1)) (1.9.3)\n",
            "Collecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Collecting PyYAML\n",
            "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m701.2/701.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-image>=0.16.1\n",
            "  Downloading scikit_image-0.19.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging>=20.0\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyparsing>=2.2.1\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow>=6.2.0\n",
            "  Downloading Pillow-9.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil>=2.7\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cycler>=0.10\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting pytz>=2017.3\n",
            "  Downloading pytz-2022.5-py2.py3-none-any.whl (500 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.7/500.7 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.1.2->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.1.2->-r requirements.txt (line 6)) (3.1.0)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.14.0-cp38-cp38-manylinux1_x86_64.whl (24.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
            "Collecting regex!=2019.12.17\n",
            "  Downloading regex-2022.9.13-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.1->-r requirements.txt (line 3)) (1.11.0)\n",
            "Collecting networkx>=2.2\n",
            "  Downloading networkx-2.8.7-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tifffile>=2019.7.26\n",
            "  Downloading tifffile-2022.10.10-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.3/210.3 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyWavelets>=1.1.1\n",
            "  Downloading PyWavelets-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imageio>=2.4.1\n",
            "  Downloading imageio-2.22.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2022.9.24-py3-none-any.whl (161 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.1/161.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<3,>=2\n",
            "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.21.1->-r requirements.txt (line 10)) (2.6)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.13.1-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.13.0-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.12.0-cp38-cp38-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.11.3-cp38-cp38-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.11.2-cp38-cp38-manylinux1_x86_64.whl (23.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.11.1-cp38-cp38-manylinux1_x86_64.whl (23.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, pytz, urllib3, typing-extensions, tqdm, regex, PyYAML, python-dateutil, pyparsing, pillow, numpy, networkx, kiwisolver, fonttools, filelock, cycler, charset-normalizer, certifi, torch, tifffile, requests, PyWavelets, pandas, packaging, opencv-python-headless, imageio, torchvision, scikit-image, matplotlib, map_boxes, huggingface-hub, transformers, timm, qudida, albumentations\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.4\n",
            "    Uninstalling numpy-1.23.4:\n",
            "      Successfully uninstalled numpy-1.23.4\n",
            "Successfully installed PyWavelets-1.4.1 PyYAML-6.0 albumentations-1.2.1 certifi-2022.9.24 charset-normalizer-2.1.1 cycler-0.11.0 filelock-3.8.0 fonttools-4.38.0 huggingface-hub-0.10.1 imageio-2.22.2 kiwisolver-1.4.4 map_boxes-1.0.5 matplotlib-3.5.1 networkx-2.8.7 numpy-1.21.4 opencv-python-headless-4.6.0.66 packaging-21.3 pandas-1.3.5 pillow-9.3.0 pyparsing-3.0.9 python-dateutil-2.8.2 pytz-2022.5 qudida-0.0.4 regex-2022.9.13 requests-2.28.1 scikit-image-0.19.3 tifffile-2022.10.10 timm-0.6.7 tokenizers-0.12.1 torch-1.10.0 torchvision-0.11.1 tqdm-4.62.3 transformers-4.21.1 typing-extensions-4.4.0 urllib3-1.26.12\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "cycler",
                  "dateutil",
                  "kiwisolver"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of found images:  17125\n",
            "Train size:  13700\n",
            "Valid size:  3425\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_3_small_384_21k.pth\" to /root/.cache/torch/hub/checkpoints/deit_3_small_384_21k.pth\n",
            "skipping pos_embed...\n",
            "skipping pos_embed...\n",
            "Epoch 1\n",
            "100% 855/857 [11:40<00:01,  1.22it/s, lr=0.00008, train_loss=4.88]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdd4ee91820>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1282, in _shutdown_workers\n",
            "    self._worker_result_queue.put((None, None))\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 89, in put\n",
            "    self._buffer.append(obj)\n",
            "KeyboardInterrupt: \n",
            "100% 855/857 [11:40<00:01,  1.22it/s, lr=0.00008, train_loss=4.88]\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 57, in <module>\n",
            "    train_eval(model,\n",
            "  File \"/content/drive/.shortcut-targets-by-id/17C-FeV89lWfoxThc1mu8REz8adMDfNfP/BTTAI_AmericanExpress1/Pix2Seq/engine.py\", line 76, in train_eval\n",
            "    train_loss = train_epoch(model, train_loader, optimizer, \n",
            "  File \"/content/drive/.shortcut-targets-by-id/17C-FeV89lWfoxThc1mu8REz8adMDfNfP/BTTAI_AmericanExpress1/Pix2Seq/engine.py\", line 24, in train_epoch\n",
            "    optimizer.step()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py\", line 65, in wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/adamw.py\", line 137, in step\n",
            "    F.adamw(params_with_grad,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/_functional.py\", line 132, in adamw\n",
            "    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "znoE4xWHCtPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 755 download_weights.sh\n",
        "!./download_weights.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_SOuobS8Km_",
        "outputId": "79f3abdd-9512-4d73-b50e-c73b37fc9c5f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qB8gmzCMq29DQbj7zhKPQ2aphGnisHkS\n",
            "To: /content/drive/.shortcut-targets-by-id/17C-FeV89lWfoxThc1mu8REz8adMDfNfP/BTTAI_AmericanExpress1/Pix2Seq/pix2seq_weights.pth\n",
            "100% 127M/127M [00:01<00:00, 109MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipykernel"
      ],
      "metadata": {
        "id": "MuxDa6v1A80s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/BTTAI_AmericanExpress1/Pix2Seq\")"
      ],
      "metadata": {
        "id": "4kaHQLYjDeQ-"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade cython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXO4h_9fI1nO",
        "outputId": "dc989fdb-35ed-440f-df82-d2ced2757e05"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cython\n",
            "  Downloading Cython-0.29.32-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cython\n",
            "Successfully installed cython-0.29.32\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python infer_single_image.py --image '/content/drive/MyDrive/BTTAI_AmericanExpress1/datasets/sampleDataset/frames_bb/datasheet001_frames_00001.jpg'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3IEdNWt_obE",
        "outputId": "27fb61b1-f043-4067-a4e6-55dcd4f7a0f0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /usr/local/lib/python3.8/dist-packages/map_boxes/compute_overlap.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "\u001b[01m\u001b[K/root/.pyxbld/temp.linux-x86_64-3.8/pyrex/map_boxes/compute_overlap.c:6:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[KPython.h: No such file or directory\n",
            " #include \u001b[01;31m\u001b[K\"Python.h\"\u001b[m\u001b[K\n",
            "          \u001b[01;31m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n",
            "Couldn't import fast version of function compute_overlap, will use slow one. Check cython intallation\n",
            "skipping pos_embed...\n",
            "skipping pos_embed...\n",
            "<All keys matched successfully>\n",
            "Figure(1200x1200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = !python infer_single_image.py --image '/content/drive/MyDrive/BTTAI_AmericanExpress1/datasets/sampleDataset/frames_bb/datasheet001_frames_00001.jpg'"
      ],
      "metadata": {
        "id": "Dzz3ihSsGRKW"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python infer_single_image.py --image './VOCdevkit/VOC2012/JPEGImages/2012_000947.jpg'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8vwiCjHAP1m",
        "outputId": "64bfa429-5377-41dc-da50-2d24354bab47"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Couldn't import fast version of function compute_overlap, will use slow one. Check cython intallation\n",
            "skipping pos_embed...\n",
            "skipping pos_embed...\n",
            "<All keys matched successfully>\n",
            "Figure(1200x1200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lve2DYPQCDeK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xe-7u1cqlcQw",
        "NFPx5FvWmEJG"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMUR9v8+vXVQWTl66p+u7SQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}